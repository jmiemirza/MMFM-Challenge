{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9842e815",
   "metadata": {},
   "source": [
    "# Document Understanding with Llava-NEXT and Structured Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6a6e10-6fb4-4675-b4ae-992b51e7f1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/MMFM-Challenge/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import huggingface_hub\n",
    "from PIL import Image, ImageOps\n",
    "from PIL.Image import Image as PILImage\n",
    "from transformers import LlavaNextProcessor\n",
    "from transformers.image_processing_utils import select_best_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380920c-e657-47b0-9de0-54e08336d35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cd39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_FORMAT = \"\"\"You are a document information extractor. You will receive an image and you must answer the user's question from the data in the image. Be exact, concise, and don't yap. Sample final answers: \"INV392834\", \"Jollibee\", \"05/11/2024\". Output in the following json format: <json_format>.\"\"\"\n",
    "PROMPT_FORMAT = \"<system_prompt> USER: <image>\\\\n<user> ASSISTANT: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de13f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-13b-hf\")\n",
    "possible_resolutions = processor.image_processor.image_grid_pinpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcd67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_resolutions += [\n",
    "    [672, 1008],\n",
    "    [1008, 672],\n",
    "    [1008, 1008],\n",
    "    [1008, 1344],\n",
    "    [1344, 1008],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca6db93e-fc62-4319-883c-611747cedce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad_image(image: PILImage) -> PILImage:\n",
    "    best_resolution = select_best_resolution(image.size, possible_resolutions)\n",
    "    print(f\"{best_resolution = }\")\n",
    "    resized_image = ImageOps.cover(image, best_resolution)\n",
    "    resized_and_padded_image = ImageOps.pad(\n",
    "        resized_image,\n",
    "        best_resolution,\n",
    "        method=processor.image_processor.resample,\n",
    "        color=(255,255,255,0),\n",
    "    )\n",
    "    return resized_and_padded_image\n",
    "\n",
    "\n",
    "def encode_local_image(image_path, resize_and_pad: bool=True):\n",
    "    # load image\n",
    "    image = Image.open(image_path)\n",
    "    if \".gif\" in image_path:\n",
    "        image = image.convert(\"RGB\")\n",
    "    if resize_and_pad:\n",
    "        image = resize_and_pad_image(image)\n",
    "        print(f\"New size: {image.size}\")\n",
    "\n",
    "    # Convert the image to a base64 string\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")  # Use the appropriate format (e.g., JPEG, PNG)\n",
    "    base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "    # add string formatting required by the endpoint\n",
    "    image_string = f\"data:image/png;base64,{base64_image}\"\n",
    "\n",
    "    return image_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95176edf-64bc-4d1b-89f7-65d87e688010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_from_question(question: str) -> str:\n",
    "    question = \" \".join(question.split(\" \"))\n",
    "    assert question.startswith(\"<image>\\nWhat is the \")\n",
    "    assert question.endswith(\" in the image?\")\n",
    "    key = question[len(\"<image>\\nWhat is the \"):-len(\" in the image?\")]\n",
    "    if key.startswith(\"[\") or key.startswith(\"â€˜\"):\n",
    "        key = key[1:]\n",
    "    if key.endswith(\"?\"):\n",
    "        key = key[:-1]\n",
    "    return (\n",
    "        key\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"_no\", \"_number\")\n",
    "        .replace(\"_$\", \"_dollars\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96df3592-fc0a-421b-8a03-88e76aa537e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_extraction_tool(key: str, max_length: int=100):\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"doc_extraction_tool\",\n",
    "            \"description\": \"Extract information from a document\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"1_reasoning\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    f\"2_{key}\": {\n",
    "                        \"type\": \"integer\" if key == \"page\" else \"string\",\n",
    "                        \"description\": \"The answer, exactly as it appears in the document.\",\n",
    "                        \"maxLength\": max_length,\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"1_reasoning\", f\"2_{key}\"],\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f33000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://bkliyhzstf7g5dyz.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "headers = {\n",
    "\t\"Accept\" : \"application/json\",\n",
    "\t\"Authorization\": f\"Bearer {huggingface_hub.get_token()}\",\n",
    "\t\"Content-Type\": \"application/json\" \n",
    "}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3adc673-89aa-42e0-856a-dc6bcbaced9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image_path, question, seed=0, max_length: int=100):\n",
    "    image_base64 = encode_local_image(image_path)\n",
    "\n",
    "    key = extract_key_from_question(question)\n",
    "    print(f\"{key = }\")\n",
    "    if key.strip() == \"\":\n",
    "        return \"This question is unanswerable.\"\n",
    "\n",
    "    tool = build_doc_extraction_tool(key, max_length)\n",
    "\n",
    "    system_prompt = SYSTEM_PROMPT_FORMAT.replace(\n",
    "        \"<json_format>\",\n",
    "        json.dumps(tool[\"function\"][\"parameters\"][\"properties\"]),\n",
    "    )\n",
    "    prompt = (\n",
    "        PROMPT_FORMAT\n",
    "        .replace(\"<system_prompt>\", system_prompt)\n",
    "        .replace(\"<image>\", f\"![]({image_base64})\")\n",
    "        .replace(\"<user>\", question[len(\"<image>\\n\"):])\n",
    "    )\n",
    "    # print(f\"{prompt = }\")\n",
    "\n",
    "    # This version of TGI uses an older version of Outlines\n",
    "    # which re-orders the keys in the JSON in alphabetical order.\n",
    "    # Hence the prefixes in the keys in the grammer\n",
    "    response = query({\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"return_full_text\": False,\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"top_p\": 0.95,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "            \"grammar\": {\n",
    "                \"type\": \"json\",\n",
    "                \"value\": tool[\"function\"][\"parameters\"],\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    print(f\"{response = }\")\n",
    "\n",
    "    return json.loads(response[0][\"generated_text\"])[f\"2_{key}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d0f95-e0b1-4c12-8d9d-b9c495a594b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c314253-ca29-409d-94ee-83e2c252d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/raw_datasets/mydoc/images/56d7d0711831b8fda7e7c7f272d407d2dd0fd4e578090c1d74761089733d6813.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path)\n",
    "image = resize_and_pad_image(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a09f-0570-48f8-89d7-f271140d7060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_inference(\n",
    "    image_path,\n",
    "    \"<image>\\nWhat is the credit status in the image?\",\n",
    "    max_length=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a7cb6-948e-4b0e-be15-278a40898f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mydoc\"\n",
    "dataset_path = os.path.join(\"data/raw_datasets\", dataset_name, \"annot_wo_answer.json\")\n",
    "print(dataset_path)\n",
    "assert os.path.exists(dataset_path)\n",
    "\n",
    "df_data = pd.read_json(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "818b75ab-750c-46db-ac23-ea519a95c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p inference_results/llava-1-6-vicuna-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a704a27-2df0-47d5-ae5e-7735b4484c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_idx = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_idx = set()\n",
    "for idx, row in df_data.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    answer_txt_path = f\"inference_results/llava-1-6-vicuna-13b-hf/{id}.txt\"\n",
    "\n",
    "    image_path = f\"data/raw_datasets/{dataset_name}/images/{row['image']}\"\n",
    "    question = row[\"conversations\"][0][\"value\"]\n",
    "\n",
    "    if os.path.exists(answer_txt_path):\n",
    "        continue\n",
    "    print(image_path)\n",
    "    print(id, idx, question)\n",
    "\n",
    "    try:\n",
    "        answer = run_inference(image_path, question, seed=7283703)\n",
    "        print(f\"{answer = }\")\n",
    "        with open(answer_txt_path, \"w\") as f:\n",
    "            f.write(str(answer))\n",
    "    except Exception as e:\n",
    "        print(\">>>>>>> ERROR\", idx, row, e, \"<<<<<<<\")\n",
    "        failed_idx.add(idx)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427974c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc773bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef49fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc68ddf-3d15-4c36-bc95-9d3e5a855323",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_data.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    answer_txt_path = f\"inference_results/llava-1-6-vicuna-13b-hf/{id}.txt\"\n",
    "    if not os.path.exists(answer_txt_path):\n",
    "        failed_idx.add(idx)\n",
    "        continue\n",
    "\n",
    "    with open(answer_txt_path, \"r\") as f:\n",
    "        answer = f.read()\n",
    "\n",
    "    if len(answer) >= 50:\n",
    "        failed_idx.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d345b-d645-4712-b0aa-8e7b69862f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237dbb13-977f-423e-abeb-d3acf887b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.iloc[list(failed_idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ec922-28ad-4a87-b59b-2fe47310a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_idx_2 = set()\n",
    "for idx, row in df_data.iloc[list(failed_idx)].iterrows():\n",
    "    id = row[\"id\"]\n",
    "    answer_txt_path = f\"inference_results/llava-1-6-vicuna-13b-hf/{id}.txt\"\n",
    "    if os.path.exists(answer_txt_path):\n",
    "        os.remove(answer_txt_path)\n",
    "    print(id, idx)\n",
    "\n",
    "    image_path = f\"data/raw_datasets/{dataset_name}/images/{row['image']}\"\n",
    "    question = row[\"conversations\"][0][\"value\"]\n",
    "\n",
    "    try:\n",
    "        answer = run_inference(image_path, question, seed=42)\n",
    "        with open(answer_txt_path, \"w\") as f:\n",
    "            f.write(answer)\n",
    "    except Exception as e:\n",
    "        print(idx, row, e)\n",
    "        failed_idx_2.add(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc41de8-07ec-4f65-a626-9968d01d90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_idx_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ce9d2-2f1a-4282-bc16-d242b42a09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.iloc[220][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fcee5-aa2f-447e-b11b-286644feb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.iloc[220][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7c344-8f1a-4bf7-8006-235f41b90a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
